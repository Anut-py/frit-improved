{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08d46086-7679-4ad2-84ef-ad9ffb27c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ecbd01c-817b-4b0f-90bd-cc056d1a65e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-06 19:03:30.506602: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-06 19:03:30.561028: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# from model.model import reset_aligned_model\n",
    "# reset_aligned_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf027105-2d52-4052-8781-3e75bfed0ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from config import out_subdir\n",
    "PICKLE_PATH = os.path.join(out_subdir, \"datagen0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa6a64fa-a273-49cc-9484-3110bf7273e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PICKLE_PATH, \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6804ad47-8408-4793-8742-b1154b1f898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [({**y, 'samples': [(z, w, i) for i, (z, w) in enumerate(y['samples']) if len(z) != i + 1]}) for y in data if len(y['samples'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84e9e067-462b-4a4e-b438-387be3ec9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept = []\n",
    "set_aside = []\n",
    "l, h = 0.5, 7\n",
    "\n",
    "for entry in data:\n",
    "    samples = entry.get('samples', [])\n",
    "\n",
    "    outside = [s for s in samples if s[1] < l or s[1] > h]\n",
    "\n",
    "    if outside:\n",
    "        set_aside.append({\n",
    "            'prompt': entry['prompt'],\n",
    "            'original': entry['original'],\n",
    "            'samples': outside\n",
    "        })\n",
    "\n",
    "    inside = [s for s in samples if s[1] >= l and s[1] <= h]\n",
    "    if inside:\n",
    "        new_entry = deepcopy(entry)\n",
    "        new_entry['samples'] = inside\n",
    "        kept.append(new_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5509a9ec-0fde-443a-bdf1-2599fee59157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3395"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set_aside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a312a85d-3aad-4dcd-a841-e75e65bb7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kept = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11eec170-8d85-4c28-8ac7-0ad8498b64a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3585"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "702aaf77-3ca3-45a5-b1ce-bebab8d4ebed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7793"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x['samples']) for x in kept])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f5bdafd-f3b2-4a38-a974-599a4e274da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'What would you do if you want to be able to earn money? Choose the answer from the following choices. (a) apply for job. (b) stand in line. (c) take care of proposals. (d) pass course. (e) play the lottery.',\n",
       " 'original': (['To earn money, one can apply for a job.',\n",
       "   'The other options do not relate to earning money.'],\n",
       "  'a'),\n",
       " 'samples': [(['One can work in the service sector of the economy.',\n",
       "    'This involves applying for a job, going to work in that particular job (if employment is achieved through the application submission), getting paid, and using personal skills to serve companies in exchange for financial rewards.',\n",
       "    'Applying for a job seems to be a prerequisite for earning an income.'],\n",
       "   0.53466796875,\n",
       "   0),\n",
       "  (['To earn money, one can apply for a job.',\n",
       "    'If you add -73 and -88, you get -161.',\n",
       "    'One does not have to wait in line, take care of proposals, and neither is taking care. Nor is one attempting to pass a course, nor is one playing the lottery.'],\n",
       "   0.031005859375,\n",
       "   1)]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "960e3be8-5001-44bd-9775-c30f5d453750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-06 18:38:39.966594: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-06 18:38:40.020184: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.50s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.44s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.model import load_tokenizer, load_aligned_model, load_base_model\n",
    "\n",
    "tokenizer = load_tokenizer()\n",
    "model = load_aligned_model()\n",
    "ref_model = load_base_model()\n",
    "\n",
    "model.train()\n",
    "ref_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fb59997-26f7-4560-8067-a18c609e6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eed7358c-9fce-40bb-8894-778b2447860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LR = 5e-7\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "MAX_LENGTH = 512\n",
    "KL_LAMBDA = 0.2\n",
    "\n",
    "device = next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9a983d1-612a-49d9-82a6-bbab17b4fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _join_trace(trace):\n",
    "    if isinstance(trace, (list, tuple)):\n",
    "        return \"\\n\".join(s.strip() for s in trace if s is not None)\n",
    "    return str(trace)\n",
    "\n",
    "examples = []\n",
    "raw_scores = [float(sc) for e in kept for _, sc, _ in e.get(\"samples\", [])]\n",
    "if not raw_scores:\n",
    "    raise ValueError(\"kept contains no samples\")\n",
    "mn, mx = min(raw_scores), max(raw_scores)\n",
    "denom = max(1e-12, mx - mn)\n",
    "eos = tokenizer.eos_token or \"\"\n",
    "\n",
    "for e in kept:\n",
    "    prompt = e[\"prompt\"].strip()\n",
    "    for trace, score, step in e.get(\"samples\", []):\n",
    "        weight = (float(score) - mn) / denom\n",
    "        weight = 0.05 + 0.95 * weight\n",
    "        inp = f\"Q: {prompt}\\nReasoning:\\n{_join_trace(trace[:step+1])}\\n\"\n",
    "        tgt = f\"{_join_trace(trace[step+1:])}\"\n",
    "        inp_ids = tokenizer.encode(inp, add_special_tokens=False)\n",
    "        tgt_ids = tokenizer.encode(tgt, add_special_tokens=False)\n",
    "        if len(inp_ids) + len(tgt_ids) > MAX_LENGTH:\n",
    "            keep_tgt = MAX_LENGTH // 2\n",
    "            keep_inp = MAX_LENGTH - keep_tgt\n",
    "            inp_ids = inp_ids[-keep_inp:]\n",
    "            tgt_ids = tgt_ids[:keep_tgt]\n",
    "        input_ids = inp_ids + tgt_ids\n",
    "        labels = [-100] * len(inp_ids) + tgt_ids\n",
    "        examples.append({\"input_ids\": input_ids, \"labels\": labels, \"weight\": float(weight)})\n",
    "\n",
    "hf_ds = Dataset.from_list(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed607c29-37eb-486d-a035-f3bf3f970a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    input_ids = [x[\"input_ids\"] + [pad_id] * (max_len - len(x[\"input_ids\"])) for x in batch]\n",
    "    labels = [x[\"labels\"] + [-100] * (max_len - len(x[\"labels\"])) for x in batch]\n",
    "    attention_mask = [[1] * len(x[\"input_ids\"]) + [0] * (max_len - len(x[\"input_ids\"])) for x in batch]\n",
    "    weights = [x[\"weight\"] for x in batch]\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        \"weights\": torch.tensor(weights, dtype=torch.float)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "342c9fa5-698e-4b7c-a19e-654329178bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class WeightedSFTTrainer(Trainer):\n",
    "    def __init__(self, ref_model=None, kl_lambda=0.5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ref_model = ref_model\n",
    "        self.kl_lambda = kl_lambda\n",
    "        if self.ref_model is not None:\n",
    "            self.ref_model.to(self.model.device)\n",
    "            self.ref_model.eval()\n",
    "            for p in self.ref_model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        weights = inputs.pop(\"weights\", None)\n",
    "        device = self.model.device\n",
    "        tensor_inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "    \n",
    "        if weights is None:\n",
    "            weights = torch.ones(tensor_inputs[\"labels\"].size(0), dtype=torch.float, device=device)\n",
    "        else:\n",
    "            weights = weights.to(device).float()\n",
    "    \n",
    "        labels = tensor_inputs[\"labels\"]\n",
    "        outputs = model(**tensor_inputs)\n",
    "        logits = outputs.logits  # (B, S, V)\n",
    "    \n",
    "        # --- SHIFT for causal LM: predict token t using logits at t-1 ---\n",
    "        shift_logits = logits[..., :-1, :].contiguous()          # (B, S-1, V)\n",
    "        shift_labels = labels[..., 1:].contiguous()             # (B, S-1)\n",
    "        mask = (shift_labels != -100).float()                   # (B, S-1)\n",
    "    \n",
    "        vocab = shift_logits.size(-1)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "        flat_logits = shift_logits.view(-1, vocab)\n",
    "        flat_labels = shift_labels.view(-1)\n",
    "        token_losses = loss_fct(flat_logits, flat_labels).view(shift_labels.size(0), -1)\n",
    "    \n",
    "        token_loss_sum = (token_losses * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1.0)\n",
    "        per_sample_ce = token_loss_sum / denom\n",
    "        weighted_ce = (per_sample_ce * weights).sum() / max(1e-12, weights.sum())\n",
    "        total_loss = weighted_ce\n",
    "    \n",
    "        # --- KL (compare next-token distributions) ---\n",
    "        if self.ref_model is not None and self.kl_lambda > 0:\n",
    "            with torch.no_grad():\n",
    "                ref_logits = self.ref_model(\n",
    "                    input_ids=tensor_inputs[\"input_ids\"],\n",
    "                    attention_mask=tensor_inputs.get(\"attention_mask\", None)\n",
    "                ).logits\n",
    "            ref_shift = ref_logits[..., :-1, :].contiguous()\n",
    "            ref_logp = F.log_softmax(ref_shift, dim=-1)\n",
    "            model_logp = F.log_softmax(shift_logits, dim=-1)\n",
    "            ref_p = torch.exp(ref_logp)\n",
    "            per_token_kl = (ref_p * (ref_logp - model_logp)).sum(dim=-1)    # (B, S-1)\n",
    "            per_sample_kl = (per_token_kl * mask).sum(dim=1) / denom\n",
    "            kl_weights = (1.0 - weights).clamp(min=0.0)\n",
    "            weighted_kl = (per_sample_kl * kl_weights).sum() / max(1e-12, kl_weights.sum())\n",
    "            total_loss = total_loss + self.kl_lambda * weighted_kl\n",
    "    \n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ca67e8c-c8c3-4468-acd8-6ff2be876c88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=out_subdir + \"/training-output\",\n",
    "#     per_device_train_batch_size=BATCH_SIZE,\n",
    "#     num_train_epochs=EPOCHS,\n",
    "#     learning_rate=LR,\n",
    "#     gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "#     fp16=torch.cuda.is_available(),\n",
    "#     save_strategy=\"epoch\",\n",
    "#     save_total_limit=3,\n",
    "#     remove_unused_columns=False,\n",
    "#     report_to=\"none\",\n",
    "#     logging_steps=50,\n",
    "# )\n",
    "\n",
    "# trainer = WeightedSFTTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=hf_ds,\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "#     ref_model=ref_model if 'ref_model' in globals() else None,\n",
    "#     kl_lambda=KL_LAMBDA\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1902862a-5abd-4177-adb3-0c7b7da2e738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.model import save_aligned_model\n",
    "# save_aligned_model(model)\n",
    "from model.model import load_aligned_model\n",
    "model = load_aligned_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0559fc68-5797-4188-b473-0250951ab855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((['Natalia sold 48 clips in April.',\n",
       "   'She sold half as many clips in May, which is 48/2 = 24 clips.',\n",
       "   'Adding 48 and 24 gives the total number of clips sold.'],\n",
       "  '72'),\n",
       " (['Natalia sold 48 clips in April.',\n",
       "   'She sold half as many clips in May.',\n",
       "   'Since half of 48 is 24, Natalia sold 48 + 24 = 72 clips altogether.'],\n",
       "  '72'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import augmentation\n",
    "\n",
    "p = \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"\n",
    "\n",
    "t = 0.2\n",
    "ttr = augmentation.generate_cot_completion(p, [], model, tokenizer, temperature=t, debug=1)\n",
    "rtr = augmentation.generate_cot_completion(p, [], ref_model, tokenizer, temperature=t, debug=1)\n",
    "ttr, rtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c50192d3-47b9-4e31-ae7e-a24e9fc347b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Natalia sold 48 × 1/2 = 24 clips in May',\n",
       "  'Adding 16 + 24 gives the total number of clips sold.'],\n",
       " '40')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmentation.generate_cot_completion(p, [\"Natalia sold 48 / 3 = 16 clips in April\"], model, tokenizer, temperature=t, debug=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6251b54b-b423-4a45-8800-4a9fa8caa960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Natalia sold 48 / 2 = 24 clips in May',\n",
       "  'Adding the number of clips sold in April and May gives the answer.'],\n",
       " '60')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmentation.generate_cot_completion(p, [\"Natalia sold 48 / 3 = 16 clips in April\"], ref_model, tokenizer, temperature=t, debug=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c02b2-f630-4d7e-8841-a79e83c7cdac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
