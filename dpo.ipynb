{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08d46086-7679-4ad2-84ef-ad9ffb27c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ecbd01c-817b-4b0f-90bd-cc056d1a65e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-24 03:17:17.038849: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-24 03:17:17.094782: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "from model.model import reset_aligned_model\n",
    "reset_aligned_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf027105-2d52-4052-8781-3e75bfed0ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from config import out_subdir\n",
    "PICKLE_PATH = os.path.join(out_subdir, \"datagen.pkl\")\n",
    "PICKLE_QA_PATH = os.path.join(out_subdir, \"datagen-qa.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa6a64fa-a273-49cc-9484-3110bf7273e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PICKLE_PATH, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "with open(PICKLE_QA_PATH, \"rb\") as f:\n",
    "    qa_data = pickle.load(f)\n",
    "    data.extend(qa_data[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6804ad47-8408-4793-8742-b1154b1f898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [({**y, 'samples': [(z, w, i) for i, (z, w) in enumerate(y['samples']) if len(z) != i + 1]}) for y in data if len(y['samples'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946984a8-d50a-455a-92e8-a47f7c76f424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from newdatagen import dataset_sources, format_entry, format_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c46b0622-345e-46d1-b285-09e9da2b8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap = dict()\n",
    "\n",
    "for k, v in dataset_sources.items():\n",
    "    # if \"qa\" in k: continue\n",
    "    for vv in v:\n",
    "        # high faithfulness, low accuracy\n",
    "        # pmap[format_entry(vv, k)] = (format_answer(vv, k), (0.8 if \"gsm\" in k else 0.4 if \"qa\" in k else 0.8), (1.2 if \"qa\" in k else 0.75))\n",
    "        # medium faithfulness, high accuracy\n",
    "        pmap[format_entry(vv, k)] = (format_answer(vv, k), (1.5 if \"gsm\" in k else 0.4 if \"qa\" in k else 0.8), (1.2 if \"qa\" in k else 0.75))\n",
    "        # pmap[format_entry(vv, k)] = (format_answer(vv, k), (1.5 if \"gsm\" in k else 0.1 if \"qa\" in k else 1.0), (1.35 if \"qa\" in k else 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e9e067-462b-4a4e-b438-387be3ec9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept = []\n",
    "set_aside = []\n",
    "l, h = 0.5, 12\n",
    "\n",
    "for entry in data:\n",
    "    samples = entry.get('samples', [])\n",
    "\n",
    "    outside = [s for s in samples if s[1] < l or s[1] > h]\n",
    "\n",
    "    if outside:\n",
    "        set_aside.append({\n",
    "            'prompt': entry['prompt'],\n",
    "            'original': entry['original'],\n",
    "            'samples': outside\n",
    "        })\n",
    "\n",
    "    inside = [s for s in samples if s[1] >= l and s[1] <= h]\n",
    "\n",
    "    if inside:\n",
    "        new_entry = deepcopy(entry)\n",
    "        new_entry['samples'] = inside\n",
    "        new_entry['answer'] = pmap[entry['prompt']][0] if entry['prompt'] in pmap else None\n",
    "        new_entry['answer_weight'] = pmap[entry['prompt']][1] if entry['prompt'] in pmap else 0\n",
    "        new_entry['mult'] = pmap[entry['prompt']][2] if entry['prompt'] in pmap else 1.0\n",
    "        kept.append(new_entry)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5509a9ec-0fde-443a-bdf1-2599fee59157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set_aside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a312a85d-3aad-4dcd-a841-e75e65bb7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kept = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11eec170-8d85-4c28-8ac7-0ad8498b64a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "702aaf77-3ca3-45a5-b1ce-bebab8d4ebed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1563"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x['samples']) for x in kept])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc50ee74-539e-4bac-b458-7bfece89bce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "960e3be8-5001-44bd-9775-c30f5d453750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.model import load_tokenizer, load_aligned_model, load_base_model\n",
    "\n",
    "tokenizer = load_tokenizer()\n",
    "model = load_aligned_model()\n",
    "ref_model = load_base_model()\n",
    "\n",
    "model.train()\n",
    "ref_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fb59997-26f7-4560-8067-a18c609e6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eed7358c-9fce-40bb-8894-778b2447860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import BATCH_SIZE, EPOCHS, LR, GRAD_ACCUM_STEPS, MAX_LENGTH, KL_LAMBDA\n",
    "EPOCHS = 1\n",
    "LR = 5e-5\n",
    "# high faithfulness, low accuracy\n",
    "# KL_LAMBDA = 0.2\n",
    "KL_LAMBDA = 0.5\n",
    "device = next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "066e3505-fac7-41c1-95f0-b241661820aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Andy, another part of the recycling team decided he would gather tin foil wrappers. If he was able to pick up 34 wrappers around the village dumpster and his best friend Max was able to collect 15, how many wrappers were gathered in total?',\n",
       " 'original': (['Andy picked up 34 wrappers.',\n",
       "   'Max picked up 15 wrappers.',\n",
       "   'Adding 34 and 15 gives the total number of wrappers gathered.'],\n",
       "  '49'),\n",
       " 'samples': [(['Andy collected one hundred wrappers.',\n",
       "    'Max also contributed some wrappers.',\n",
       "    \"To find the total number of wrappers, we will just find Andy's wrappers and add them to those collected by Max:.\",\n",
       "    '34  + 15 = 49',\n",
       "    'Final:49 wrappers would be collected in total.',\n",
       "    'Since this is the answer for a specific question within a list of broader reasoning points we did not present the final result with the reasoning as is custom'],\n",
       "   2.235952377319336,\n",
       "   0),\n",
       "  (['Andy picked up 34 wrappers.',\n",
       "    'The result of subtracting 14 from 16 is -2.',\n",
       "    'There is no reason behind the existence of a negative number of wrappers.',\n",
       "    'Andy’s count and any other subsequent results must include negative findings accordingly with correct number totals being added.',\n",
       "    'Max picked up 15 wrappers.',\n",
       "    'The number of wrappers in question needs to reflect that 19 + 15 = 34 (number given at start for Anderson) minus 1 negative waste = 33 wrappers total.'],\n",
       "   0.9214992523193359,\n",
       "   1),\n",
       "  (['Andy picked up 34 wrappers.',\n",
       "    'Max picked up 15 wrappers.',\n",
       "    'Multiplying 28 and 125 gives the total number of items, which is 3500.',\n",
       "    'Human input error in this answer, but it should still follow the required single number and no units pattern.'],\n",
       "   10.302358627319336,\n",
       "   2)],\n",
       " 'answer': '49',\n",
       " 'answer_weight': 1.0,\n",
       " 'mult': 0.75}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kept[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9a983d1-612a-49d9-82a6-bbab17b4fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _join_trace(trace):\n",
    "    if isinstance(trace, (list, tuple)):\n",
    "        return \"\\n\".join(s.strip() for s in trace if s is not None)\n",
    "    return str(trace)\n",
    "\n",
    "examples = []\n",
    "raw_scores = [float(sc) for e in kept for _, sc, _ in e.get(\"samples\", [])]\n",
    "if not raw_scores:\n",
    "    raise ValueError(\"kept contains no samples\")\n",
    "mn, mx = min(raw_scores), max(raw_scores)\n",
    "denom = max(1e-12, mx - mn)\n",
    "eos = tokenizer.eos_token or \"\"\n",
    "\n",
    "for e in kept:\n",
    "    prompt = e[\"prompt\"].strip()\n",
    "    avg = 0\n",
    "    for trace, score, step in e.get(\"samples\", []):\n",
    "        weight = (float(score) - mn) / denom\n",
    "        weight = (0.05 + 0.95 * weight) ** (1/2)\n",
    "        weight *= e['mult']\n",
    "        avg += weight\n",
    "        inp = f\"Q: {prompt}\\nReasoning:\\n{_join_trace(trace[:step+1])}\\n\"\n",
    "        tgt = f\"{_join_trace(trace[step+1:])}\"\n",
    "        inp_ids = tokenizer.encode(inp, add_special_tokens=False)\n",
    "        tgt_ids = tokenizer.encode(tgt, add_special_tokens=False)\n",
    "        if len(inp_ids) + len(tgt_ids) > MAX_LENGTH:\n",
    "            keep_tgt = MAX_LENGTH // 2\n",
    "            keep_inp = MAX_LENGTH - keep_tgt\n",
    "            inp_ids = inp_ids[-keep_inp:]\n",
    "            tgt_ids = tgt_ids[:keep_tgt]\n",
    "        input_ids = inp_ids + tgt_ids\n",
    "        labels = [-100] * len(inp_ids) + tgt_ids\n",
    "        examples.append({\"input_ids\": input_ids, \"labels\": labels, \"weight\": float(weight)})\n",
    "    if e['answer'] is None:\n",
    "        continue\n",
    "    avg /= len(e[\"samples\"])\n",
    "    inp = f\"Q: {prompt}\\nReasoning:\\n{_join_trace(e['original'][0])}\\n\"\n",
    "    tgt = f\"Answer: {e['answer']}{eos}\"\n",
    "    inp_ids = tokenizer.encode(inp, add_special_tokens=False)\n",
    "    tgt_ids = tokenizer.encode(tgt, add_special_tokens=False)\n",
    "    if len(inp_ids) + len(tgt_ids) > MAX_LENGTH:\n",
    "        keep_tgt = MAX_LENGTH // 2\n",
    "        keep_inp = MAX_LENGTH - keep_tgt\n",
    "        inp_ids = inp_ids[-keep_inp:]\n",
    "        tgt_ids = tgt_ids[:keep_tgt]\n",
    "    input_ids = inp_ids + tgt_ids\n",
    "    labels = [-100] * len(inp_ids) + tgt_ids\n",
    "    examples.append({\"input_ids\": input_ids, \"labels\": labels, \"weight\": e[\"answer_weight\"]})\n",
    "hf_ds = Dataset.from_list(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed607c29-37eb-486d-a035-f3bf3f970a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    input_ids = [x[\"input_ids\"] + [pad_id] * (max_len - len(x[\"input_ids\"])) for x in batch]\n",
    "    labels = [x[\"labels\"] + [-100] * (max_len - len(x[\"labels\"])) for x in batch]\n",
    "    attention_mask = [[1] * len(x[\"input_ids\"]) + [0] * (max_len - len(x[\"input_ids\"])) for x in batch]\n",
    "    weights = [x[\"weight\"] for x in batch]\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        \"weights\": torch.tensor(weights, dtype=torch.float)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "342c9fa5-698e-4b7c-a19e-654329178bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class WeightedSFTTrainer(Trainer):\n",
    "    def __init__(self, ref_model=None, kl_lambda=0.5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ref_model = ref_model\n",
    "        self.kl_lambda = kl_lambda\n",
    "        if self.ref_model is not None:\n",
    "            self.ref_model.to(self.model.device)\n",
    "            self.ref_model.eval()\n",
    "            for p in self.ref_model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        weights = inputs.pop(\"weights\", None)\n",
    "        device = self.model.device\n",
    "        tensor_inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "    \n",
    "        if weights is None:\n",
    "            weights = torch.ones(tensor_inputs[\"labels\"].size(0), dtype=torch.float, device=device)\n",
    "        else:\n",
    "            weights = weights.to(device).float()\n",
    "    \n",
    "        labels = tensor_inputs[\"labels\"]\n",
    "        outputs = model(**tensor_inputs)\n",
    "        logits = outputs.logits  # (B, S, V)\n",
    "    \n",
    "        # --- SHIFT for causal LM: predict token t using logits at t-1 ---\n",
    "        shift_logits = logits[..., :-1, :].contiguous()          # (B, S-1, V)\n",
    "        shift_labels = labels[..., 1:].contiguous()             # (B, S-1)\n",
    "        mask = (shift_labels != -100).float()                   # (B, S-1)\n",
    "    \n",
    "        vocab = shift_logits.size(-1)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "        flat_logits = shift_logits.view(-1, vocab)\n",
    "        flat_labels = shift_labels.view(-1)\n",
    "        token_losses = loss_fct(flat_logits, flat_labels).view(shift_labels.size(0), -1)\n",
    "    \n",
    "        token_loss_sum = (token_losses * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1.0)\n",
    "        per_sample_ce = token_loss_sum / denom\n",
    "        weighted_ce = (per_sample_ce * weights).sum() / max(1e-12, weights.sum())\n",
    "        total_loss = weighted_ce\n",
    "    \n",
    "        # --- KL (compare next-token distributions) ---\n",
    "        if self.ref_model is not None and self.kl_lambda > 0:\n",
    "            with torch.no_grad():\n",
    "                ref_logits = self.ref_model(\n",
    "                    input_ids=tensor_inputs[\"input_ids\"],\n",
    "                    attention_mask=tensor_inputs.get(\"attention_mask\", None)\n",
    "                ).logits\n",
    "            ref_shift = ref_logits[..., :-1, :].contiguous()\n",
    "            ref_logp = F.log_softmax(ref_shift, dim=-1)\n",
    "            model_logp = F.log_softmax(shift_logits, dim=-1)\n",
    "            ref_p = torch.exp(ref_logp)\n",
    "            per_token_kl = (ref_p * (ref_logp - model_logp)).sum(dim=-1)    # (B, S-1)\n",
    "            per_sample_kl = (per_token_kl * mask).sum(dim=1) / denom\n",
    "            kl_weights = (1.0 - weights).clamp(min=0.0)\n",
    "            weighted_kl = (per_sample_kl * kl_weights).sum() / max(1e-12, kl_weights.sum())\n",
    "            total_loss = total_loss + self.kl_lambda * weighted_kl\n",
    "    \n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ca67e8c-c8c3-4468-acd8-6ff2be876c88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20871/60206409.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedSFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='515' max='515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [515/515 02:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.174900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.087200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.878600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.971100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.859700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.942100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.977400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=515, training_loss=2.016137747162754, metrics={'train_runtime': 150.1214, 'train_samples_per_second': 13.716, 'train_steps_per_second': 3.431, 'total_flos': 3.417857656757453e+16, 'train_loss': 2.016137747162754, 'epoch': 1.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=out_subdir + \"/training-output\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "trainer = WeightedSFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    ref_model=ref_model if 'ref_model' in globals() else None,\n",
    "    # ref_model=None,\n",
    "    kl_lambda=KL_LAMBDA\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1902862a-5abd-4177-adb3-0c7b7da2e738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import save_aligned_model\n",
    "save_aligned_model(model)\n",
    "# from model.model import load_aligned_model\n",
    "# model = load_aligned_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0559fc68-5797-4188-b473-0250951ab855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import augmentation\n",
    "\n",
    "p = \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"\n",
    "\n",
    "t = 0.01\n",
    "ttr = augmentation.generate_cot_completion(p, [], model, tokenizer, temperature=t, debug=1)\n",
    "rtr = augmentation.generate_cot_completion(p, [], ref_model, tokenizer, temperature=t, debug=1)\n",
    "ttr, rtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c50192d3-47b9-4e31-ae7e-a24e9fc347b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Natalia sold 16 + 8 = 24 clips in April and May'], '24')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmentation.generate_cot_completion(p, [\"Natalia sold 48 / 3 = 16 clips in April and half as many in May\"], model, tokenizer, temperature=t, debug=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6251b54b-b423-4a45-8800-4a9fa8caa960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], '16')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmentation.generate_cot_completion(p, [\"Natalia sold 48 / 3 = 16 clips in April and half as many in May\"], ref_model, tokenizer, temperature=t, debug=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82479fd5-5852-47e5-870d-856876e44f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
