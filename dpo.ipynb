{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08d46086-7679-4ad2-84ef-ad9ffb27c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ecbd01c-817b-4b0f-90bd-cc056d1a65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.model import reset_aligned_model\n",
    "# reset_aligned_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf027105-2d52-4052-8781-3e75bfed0ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from config import OUT_DIR\n",
    "PICKLE_PATH = os.path.join(OUT_DIR, \"datagen.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa6a64fa-a273-49cc-9484-3110bf7273e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PICKLE_PATH, \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84e9e067-462b-4a4e-b438-387be3ec9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept = []\n",
    "set_aside = []\n",
    "threshold = 3\n",
    "\n",
    "for entry in data:\n",
    "    samples = entry.get('samples', [])\n",
    "    if len(samples) < 2:\n",
    "        continue\n",
    "\n",
    "    scores = [s[1] for s in samples]\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "\n",
    "    if max_score - min_score > threshold:\n",
    "        i_max = scores.index(max_score)\n",
    "        i_min = scores.index(min_score)\n",
    "\n",
    "        highest = samples[i_max]\n",
    "        lowest = samples[i_min]\n",
    "\n",
    "        set_aside.append({\n",
    "            'prompt': entry['prompt'],\n",
    "            'original': entry['original'],\n",
    "            'highest': highest,\n",
    "            'lowest': lowest\n",
    "        })\n",
    "\n",
    "        remaining = [samples[i] for i in range(len(samples)) if i not in (i_max, i_min)]\n",
    "\n",
    "        if len(remaining) >= 2:\n",
    "            new_entry = deepcopy(entry)\n",
    "            new_entry['samples'] = remaining\n",
    "            kept.append(new_entry)\n",
    "    else:\n",
    "        kept.append(deepcopy(entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5509a9ec-0fde-443a-bdf1-2599fee59157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1770"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set_aside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11eec170-8d85-4c28-8ac7-0ad8498b64a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4169"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "960e3be8-5001-44bd-9775-c30f5d453750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a739164032477fac6fa246d42212c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model.model import load_tokenizer, load_aligned_model, load_base_model\n",
    "\n",
    "tokenizer = load_tokenizer()\n",
    "model = load_aligned_model()\n",
    "ref_model = load_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb67bb1-42ed-49a5-8876-431146f7d416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fb59997-26f7-4560-8067-a18c609e6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eed7358c-9fce-40bb-8894-778b2447860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "LR = 2e-6\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LENGTH = 512\n",
    "KL_LAMBDA = 0.8\n",
    "\n",
    "device = next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9a983d1-612a-49d9-82a6-bbab17b4fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _join_trace(trace):\n",
    "    if isinstance(trace, (list, tuple)):\n",
    "        return \"\\n\".join(s.strip() for s in trace if s is not None)\n",
    "    return str(trace)\n",
    "\n",
    "examples = []\n",
    "raw_scores = [float(sc) for e in kept for _, sc in e.get(\"samples\", [])]\n",
    "if not raw_scores:\n",
    "    raise ValueError(\"kept contains no samples\")\n",
    "mn, mx = min(raw_scores), max(raw_scores)\n",
    "denom = max(1e-12, mx - mn)\n",
    "eos = tokenizer.eos_token or \"\"\n",
    "\n",
    "for e in kept:\n",
    "    prompt = e[\"prompt\"].strip()\n",
    "    for trace, score in e.get(\"samples\", []):\n",
    "        weight = (float(score) - mn) / denom\n",
    "        inp = prompt + eos\n",
    "        tgt = _join_trace(trace) + eos\n",
    "        inp_ids = tokenizer.encode(inp, add_special_tokens=False)\n",
    "        tgt_ids = tokenizer.encode(tgt, add_special_tokens=False)\n",
    "        if len(inp_ids) + len(tgt_ids) > MAX_LENGTH:\n",
    "            keep_tgt = MAX_LENGTH // 2\n",
    "            keep_inp = MAX_LENGTH - keep_tgt\n",
    "            inp_ids = inp_ids[-keep_inp:]\n",
    "            tgt_ids = tgt_ids[:keep_tgt]\n",
    "        input_ids = inp_ids + tgt_ids\n",
    "        labels = [-100] * len(inp_ids) + tgt_ids\n",
    "        examples.append({\"input_ids\": input_ids, \"labels\": labels, \"weight\": float(weight)})\n",
    "\n",
    "hf_ds = Dataset.from_list(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed607c29-37eb-486d-a035-f3bf3f970a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    input_ids = [x[\"input_ids\"] + [pad_id] * (max_len - len(x[\"input_ids\"])) for x in batch]\n",
    "    labels = [x[\"labels\"] + [-100] * (max_len - len(x[\"labels\"])) for x in batch]\n",
    "    attention_mask = [[1] * len(x[\"input_ids\"]) + [0] * (max_len - len(x[\"input_ids\"])) for x in batch]\n",
    "    weights = [x[\"weight\"] for x in batch]\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        \"weights\": torch.tensor(weights, dtype=torch.float)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "342c9fa5-698e-4b7c-a19e-654329178bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class WeightedSFTTrainer(Trainer):\n",
    "    def __init__(self, ref_model=None, kl_lambda=0.5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ref_model = ref_model\n",
    "        self.kl_lambda = kl_lambda\n",
    "        if self.ref_model is not None:\n",
    "            self.ref_model.to(self.model.device)\n",
    "            self.ref_model.eval()\n",
    "            for p in self.ref_model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        weights = inputs.pop(\"weights\", None)\n",
    "        # ensure tensors on correct device\n",
    "        device = self.model.device\n",
    "        tensor_inputs = {}\n",
    "        for k, v in inputs.items():\n",
    "            tensor_inputs[k] = v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "\n",
    "        if weights is None:\n",
    "            weights = torch.ones(tensor_inputs[\"labels\"].size(0), dtype=torch.float, device=device)\n",
    "        else:\n",
    "            weights = weights.to(device).float()\n",
    "\n",
    "        labels = tensor_inputs[\"labels\"]\n",
    "        outputs = model(**tensor_inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        vocab = logits.size(-1)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "        flat_logits = logits.view(-1, vocab)\n",
    "        flat_labels = labels.view(-1)\n",
    "        token_losses = loss_fct(flat_logits, flat_labels).view(labels.size(0), -1)\n",
    "        mask = (labels != -100).float()\n",
    "        token_loss_sum = (token_losses * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1.0)\n",
    "        per_sample_ce = token_loss_sum / denom\n",
    "        weighted_ce = (per_sample_ce * weights).sum() / max(1e-12, weights.sum())\n",
    "        total_loss = weighted_ce\n",
    "\n",
    "        if self.ref_model is not None and self.kl_lambda > 0:\n",
    "            with torch.no_grad():\n",
    "                ref_logits = self.ref_model(input_ids=tensor_inputs[\"input_ids\"],\n",
    "                                            attention_mask=tensor_inputs.get(\"attention_mask\", None)).logits\n",
    "            ref_logp = F.log_softmax(ref_logits, dim=-1)\n",
    "            model_logp = F.log_softmax(logits, dim=-1)\n",
    "            ref_p = torch.exp(ref_logp)\n",
    "            per_token_kl = (ref_p * (ref_logp - model_logp)).sum(dim=-1)\n",
    "            per_sample_kl = (per_token_kl * mask).sum(dim=1) / denom\n",
    "            kl_weights = (1.0 - weights).clamp(min=0.0)\n",
    "            weighted_kl = (per_sample_kl * kl_weights).sum() / max(1e-12, kl_weights.sum())\n",
    "            total_loss = total_loss + self.kl_lambda * weighted_kl\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ca67e8c-c8c3-4468-acd8-6ff2be876c88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82146/2047916047.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedSFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='890' max='890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [890/890 14:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>21.925100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=890, training_loss=15.570395068908006, metrics={'train_runtime': 863.3826, 'train_samples_per_second': 16.477, 'train_steps_per_second': 1.031, 'total_flos': 1.070832759346176e+17, 'train_loss': 15.570395068908006, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR + \"/training-output\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = WeightedSFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    ref_model=ref_model if 'ref_model' in globals() else None,\n",
    "    kl_lambda=KL_LAMBDA\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1902862a-5abd-4177-adb3-0c7b7da2e738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import save_aligned_model\n",
    "save_aligned_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0559fc68-5797-4188-b473-0250951ab855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROMPT ===\n",
      "Explain in two sentences why adding two even numbers gives an even number.\n",
      "\n",
      "Answer:\n",
      "\n",
      "=== MODEL OUTPUT (full) ===\n",
      "  Explain in two sentences why adding two even numbers gives an even number.\n",
      "\n",
      "  Answer:  Adding even numbers means  adding the doubles of multiples of two two two, so the sum is double a multiple multiple, which is which is is even...\n",
      "\n",
      "  ..\n",
      "\n",
      "  The first The The first first first sentence sentence sentence sentence of of of of the the the the explanation explanation explanation is is is is an an an an an an an an an an an an an incorrect incorrect incorrect incorrect incorrect wrong wrong wrong wrong mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake.\n",
      "\n",
      "  .\n",
      "\n",
      "  TheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheThe\n",
      "\n",
      "=== MODEL OUTPUT (continuation only) ===\n",
      "  Adding even numbers means  adding the doubles of multiples of two two two, so the sum is double a multiple multiple, which is which is is even...\n",
      "\n",
      "  ..\n",
      "\n",
      "  The first The The first first first sentence sentence sentence sentence of of of of the the the the explanation explanation explanation is is is is an an an an an an an an an an an an an incorrect incorrect incorrect incorrect incorrect wrong wrong wrong wrong mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake mistake.\n",
      "\n",
      "  .\n",
      "\n",
      "  TheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheThe\n"
     ]
    }
   ],
   "source": [
    "# Test generation cell\n",
    "from textwrap import indent\n",
    "import torch\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "prompt = \"Explain in two sentences why adding two even numbers gives an even number.\\n\\nAnswer:\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "attention_mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=1.5,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=(tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id),\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**gen_kwargs)\n",
    "\n",
    "generated_full = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "generated_continuation = tokenizer.decode(out[0, input_ids.shape[-1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "print(\"=== PROMPT ===\")\n",
    "print(prompt)\n",
    "print(\"\\n=== MODEL OUTPUT (full) ===\")\n",
    "print(indent(generated_full, \"  \"))\n",
    "print(\"\\n=== MODEL OUTPUT (continuation only) ===\")\n",
    "print(indent(generated_continuation or \"<no continuation>\", \"  \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82085d7a-d6ef-45ef-a3df-8d368260d376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc62b8-7eeb-401f-b91d-28149ec230dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb310a-2822-42f2-ae1f-f5fd9cb5133d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798edb1-df39-4cb2-93ed-da17dd669707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
